{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.layers as L\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig():\n",
    "    epochs = 5\n",
    "    batch_size= 128\n",
    "    learningRate=0.001\n",
    "    valRate=0.1\n",
    "    loss=['sparse_categorical_crossentropy']\n",
    "    metrics=['accuracy']\n",
    "    optimizer='adam'\n",
    "    \n",
    "class WordEmbedding():\n",
    "    sequenceLength=256\n",
    "    wordCount=10000\n",
    "    wordDim=10\n",
    "\n",
    "class DataSourceConfig():\n",
    "    source=[\"./dataset/imdb_train_data.npy\",\n",
    "           \"./dataset/imdb_test_data.npy\",\n",
    "           \"./dataset/imdb_train_labels.npy\",\n",
    "           \"./dataset/imdb_test_labels.npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADLSTMModel(K.Model):\n",
    "    def __init__(self,config,wordEmbedding):\n",
    "        super(ADLSTMModel, self).__init__()\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.config=config\n",
    "        self.emb=wordEmbedding\n",
    "        self.embedding=L.Embedding(input_dim=self.emb.wordCount,output_dim=self.emb.wordDim,input_length=self.emb.sequenceLength)\n",
    "        self.lstm=L.LSTM(32,dropout=0.4)\n",
    "        self.output_layer=L.Dense(2,activation=\"softmax\")\n",
    "        \n",
    "    def get_adv_r(self,emb,y_true,rate=0.02):\n",
    "        # 转化为常量意味着在tape中不用求导\n",
    "        #emb=tf.convert_to_tensor(emb)\n",
    "        with tf.GradientTape() as tp:\n",
    "            #太难了，这是个tensor得watch才能记录\n",
    "            tp.watch(emb)\n",
    "            y=self.lstm(emb)\n",
    "            y_pred=self.output_layer(y)\n",
    "            loss=loss_fn_(y_true,y_pred)\n",
    "        g=tp.gradient(y,emb)\n",
    "        g=rate*tf.nn.l2_normalize(g,axis=2)\n",
    "        #g=tf.convert_to_tensor(g)\n",
    "        return g,loss\n",
    "    \n",
    "    def call(self, x,training=None,y_true=None):\n",
    "        emb=self.embedding(x)\n",
    "        #emb=tf.nn.l2_normalize(emb,axis=2)\n",
    "        g=0\n",
    "        if training is not None and y_true is not None:\n",
    "            #g,loss=self.get_adv_r(emb,y_true)\n",
    "            #self.add_loss(loss)\n",
    "            pass\n",
    "        \n",
    "        ad_x=self.lstm(emb+g)\n",
    "        output=self.output_layer(ad_x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class AD_LSTM():\n",
    "    def __init__(self,config,wordEmbedding):\n",
    "        \n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.config=config\n",
    "        self.emb=wordEmbedding\n",
    "        self.name=\"AD_LSTM\"\n",
    "        \n",
    "    def design_model(self,hiden_size=32,with_fc=False):\n",
    "        \n",
    "        self.model=ADLSTMModel(self.config,self.emb)\n",
    "\n",
    "        #model.summary()\n",
    "        return model\n",
    "    \n",
    "    def compile_model(self):\n",
    "        #assert()\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "            return\n",
    "        self.model.compile(optimizer=self.config.optimizer,loss=self.config.loss,metrics=self.config.metrics)\n",
    "\n",
    "    def train_model(self,x,y):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "            return\n",
    "        self.compile_model()\n",
    "        self.history=self.model.fit(x=x,y=y,batch_size=self.config.batch_size,epochs=self.config.epochs,validation_split=self.config.valRate)\n",
    "        #self.model.compile()\n",
    "        \n",
    "    def train_model_custom(self,x,y):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.1)\n",
    "        print(x_train.shape)\n",
    "        print(x_val.shape)\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(self.config.batch_size)\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(self.config.batch_size)\n",
    "\n",
    "        #loss\n",
    "        loss_fn=K.losses.get(self.config.loss[0])\n",
    "        #opt\n",
    "        optimizer=K.optimizers.Adam()\n",
    "        #metric\n",
    "        train_acc_metric=K.metrics.SparseCategoricalAccuracy()#self.config.metrics[0])\n",
    "        val_acc_metric  =K.metrics.SparseCategoricalAccuracy()#self.config.metrics[0])\n",
    "        \n",
    "        for times in range(self.config.epochs):\n",
    "            step=0\n",
    "            # 训练过程\n",
    "            for x_batch_train, y_batch_train in train_dataset:\n",
    "                with tf.GradientTape() as tp:\n",
    "                    logits=self.model(x_batch_train)\n",
    "                    loss=loss_fn(y_batch_train,logits)\n",
    "                grads=tp.gradient(loss,self.model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "                train_acc_metric.update_state(y_batch_train,logits)\n",
    "                \n",
    "                if step % 50 == 0:\n",
    "                    print('\\r','Training loss (for one batch) at step %s: %s' % (step, float(tf.reduce_mean(loss))),end=\"\",flush=True)\n",
    "                    #print('Seen so far: %s samples' % ((step + 1) * self.config.batch_size))\n",
    "                step+=1\n",
    "            print()\n",
    "            # 每个周期结束看一次\n",
    "            train_acc = train_acc_metric.result()\n",
    "            print('Training     acc over epoch %s: %s' % (times,float(train_acc)))\n",
    "            # Reset training metrics at the end of each epoch\n",
    "            train_acc_metric.reset_states()\n",
    "            \n",
    "            #验证过程\n",
    "            for x_batch_val, y_batch_val in val_dataset:\n",
    "                val_logits=self.model(x_batch_val)\n",
    "                val_acc_metric.update_state(y_batch_val,val_logits)\n",
    "            \n",
    "            val_acc=val_acc_metric.result()\n",
    "            print('Training val_acc over epoch %s: %s '%(times,float(val_acc)))\n",
    "            val_acc_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    def save_model(self):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        pass\n",
    "    \n",
    "    def eval_model(self):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        pass\n",
    "    \n",
    "    def view_train(self):\n",
    "        if not self.history:\n",
    "            print(\"Model has not been trained, train it first\")\n",
    "            return\n",
    "        \n",
    "        train=self.history.history[\"loss\"]\n",
    "        valid=self.history.history[\"val_loss\"]\n",
    "        name='loss'\n",
    "        plt.title('The %s with epoch runs'%name,fontsize=30)\n",
    "        plt.xlabel('epoch',fontsize=20)\n",
    "        plt.ylabel(name,fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.plot(train,label=name)\n",
    "        plt.plot(valid,label=\"val_\"+name)\n",
    "        plt.legend()\n",
    "        plt.gcf().set_size_inches(15,4)\n",
    "        plt.show()\n",
    "    #model=K.models.Model(inputs=[input_layer],outputs=[output_layer])\n",
    "    #model.compile('adam',loss='categorical_crossentropy',metrics=metrics)\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AD_LSTM at 0x14a481ad0>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config=TrainingConfig()\n",
    "word_embedding=WordEmbedding()\n",
    "model=AD_LSTM(train_config,word_embedding)\n",
    "model.design_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self,config):\n",
    "        self.config=config\n",
    "    def load(self):\n",
    "        L=[]\n",
    "        for item in self.config.source:\n",
    "            L.append(np.load(item,allow_pickle=True, fix_imports=True))\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Dataset(DataSourceConfig)\n",
    "train_data,test_data,train_labels,test_labels=data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train_model_custom(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.train_model(x=train_data,y=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

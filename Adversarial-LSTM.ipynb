{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.layers as L\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig():\n",
    "    epochs = 50\n",
    "    batch_size= 128\n",
    "    learningRate=0.001\n",
    "    valRate=0.1\n",
    "    loss=['sparse_categorical_crossentropy']\n",
    "    metrics=['accuracy']\n",
    "    optimizer='adam'\n",
    "    \n",
    "class WordEmbedding():\n",
    "    sequenceLength=256\n",
    "    wordCount=10000\n",
    "    wordDim=10\n",
    "\n",
    "class DataSourceConfig():\n",
    "    source=[\"./dataset/imdb_train_data.npy\",\n",
    "           \"./dataset/imdb_test_data.npy\",\n",
    "           \"./dataset/imdb_train_labels.npy\",\n",
    "           \"./dataset/imdb_test_labels.npy\"]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 192,
=======
   "execution_count": 510,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=tf.Variable(3.)\n",
    "with tf.GradientTape() as tp:\n",
    "    y=x+3\n",
    "    tf.stop_gradient(x)\n",
    "f=tp.gradient(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=928923, shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADLSTMModel(K.Model):\n",
    "    def __init__(self,config,wordEmbedding,freq):\n",
    "        super(ADLSTMModel, self).__init__()\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.config=config\n",
    "        self.emb=wordEmbedding\n",
    "        self.embedding=L.Embedding(input_dim=self.emb.wordCount,output_dim=self.emb.wordDim,input_length=self.emb.sequenceLength)\n",
    "        self.lstm=L.LSTM(32,dropout=0.4)\n",
    "        self.output_layer=L.Dense(2,activation=\"softmax\")\n",
    "        self.freq=freq.reshape((-1,1)).astype('float32')\n",
    "        self.loss_fn=K.losses.get(self.config.loss[0])\n",
    "    def emb_normalize(self,emb):\n",
    "        E=tf.reduce_sum(self.freq*emb,axis=0)\n",
    "        V=tf.sqrt(tf.maximum(tf.reduce_sum(self.freq*(emb-E)**2,axis=0),1e-12))\n",
    "        #print(V)\n",
    "        return E,V\n",
    "\n",
    "    def get_adv_r(self,emb,y_true,rate=0.02):\n",
    "        # 转化为常量意味着在tape中不用求导\n",
    "        #emb=tf.convert_to_tensor(emb)\n",
    "        x=emb\n",
    "        with tf.GradientTape() as tp:\n",
    "            #太难了，这是个tensor得watch才能记录\n",
    "            tp.watch(x)\n",
    "            y=self.lstm(x)\n",
    "            y_pred=self.output_layer(y)\n",
    "            loss=self.loss_fn(y_true,y_pred)\n",
<<<<<<< Updated upstream
    "            \n",
    "        #\n",
    "        g=tp.gradient(loss,x)\n",
    "        g=rate*tf.nn.l2_normalize(tf.stop_gradient(g),axis=2)\n",
=======
    "        \n",
    "        g=tp.gradient(loss,emb)\n",
    "        g=rate*tf.nn.l2_normalize(g,axis=2)\n",
>>>>>>> Stashed changes
    "        #g=tf.convert_to_tensor(g)\n",
    "        return g,loss\n",
    "    \n",
    "    def call(self, x,training=None,y_true=None):\n",
    "        emb=self.embedding(x)\n",
    "        E,V=self.emb_normalize(self.embedding.embeddings)\n",
    "        emb=(emb-E)/V\n",
    "        #emb=tf.nn.l2_normalize(emb,axis=2)\n",
    "        g=0\n",
    "        if training is not None and y_true is not None:\n",
    "            g,loss=self.get_adv_r(emb,y_true)\n",
    "            self.add_loss(loss)\n",
<<<<<<< Updated upstream
    "            #x=self.lstm(emb)\n",
    "            #output_norm=self.output_layer(x);\n",
    "\n",
=======
    "            x=self.lstm(emb)\n",
    "            output_norm=self.output_layer(x);\n",
    "            pass\n",
>>>>>>> Stashed changes
    "        \n",
    "        ad_x=self.lstm(emb+g)\n",
    "        output=self.output_layer(ad_x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 205,
=======
   "execution_count": 493,
>>>>>>> Stashed changes
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class AD_LSTM():\n",
    "    def __init__(self,config,wordEmbedding,freq):\n",
    "        \n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.config=config\n",
    "        self.emb=wordEmbedding\n",
    "        self.name=\"AD_LSTM\"\n",
    "        self.freq=freq\n",
    "        \n",
    "    def design_model(self,hiden_size=32,with_fc=False):\n",
    "        \n",
    "        self.model=ADLSTMModel(self.config,self.emb,self.freq)\n",
    "\n",
    "        #model.summary()\n",
    "        return self.model\n",
    "    \n",
    "    def compile_model(self):\n",
    "        #assert()\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "            return\n",
    "        self.model.compile(optimizer=self.config.optimizer,loss=self.config.loss,metrics=self.config.metrics)\n",
    "\n",
    "    def train_model(self,x,y):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "            return\n",
    "        self.compile_model()\n",
    "        self.history=self.model.fit(x=x,y=y,batch_size=self.config.batch_size,epochs=self.config.epochs,validation_split=self.config.valRate)\n",
    "        #self.model.compile()\n",
    "        \n",
    "    def train_model_custom(self,x,y,adv=True):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.1,shuffle=False)\n",
    "        print(x_train.shape)\n",
    "        print(x_val.shape)\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(self.config.batch_size)\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(self.config.batch_size)\n",
    "\n",
    "        #loss\n",
    "        loss_fn=K.losses.get(self.config.loss[0])\n",
    "        #opt\n",
    "        optimizer=K.optimizers.Adam()\n",
    "        #metric\n",
    "        train_acc_metric=K.metrics.SparseCategoricalAccuracy()#self.config.metrics[0])\n",
    "        val_acc_metric  =K.metrics.SparseCategoricalAccuracy()#self.config.metrics[0])\n",
    "        \n",
    "        for times in range(self.config.epochs):\n",
    "            step=0\n",
    "            # 训练过程\n",
    "            for x_batch_train, y_batch_train in train_dataset:\n",
    "                with tf.GradientTape() as tp:\n",
    "                    logits=self.model(x_batch_train,training=adv,y_true=y_batch_train)\n",
    "                    loss=loss_fn(y_batch_train,logits)\n",
    "                grads=tp.gradient(loss,self.model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "                train_acc_metric.update_state(y_batch_train,logits)\n",
    "                \n",
    "                if step % 50 == 0:\n",
    "                    print('\\r','Training loss (for one batch) at step %s: %s' % (step, float(tf.reduce_mean(loss))),end=\"\",flush=True)\n",
    "                    #print('Seen so far: %s samples' % ((step + 1) * self.config.batch_size))\n",
    "                step+=1\n",
    "            print()\n",
    "            # 每个周期结束看一次\n",
    "            train_acc = train_acc_metric.result()\n",
    "            print('Training     acc over epoch %s: %s' % (times,float(train_acc)))\n",
    "            # Reset training metrics at the end of each epoch\n",
    "            train_acc_metric.reset_states()\n",
    "            \n",
    "            #验证过程\n",
    "            for x_batch_val, y_batch_val in val_dataset:\n",
    "                val_logits=self.model(x_batch_val)\n",
    "                val_acc_metric.update_state(y_batch_val,val_logits)\n",
    "            \n",
    "            val_acc=val_acc_metric.result()\n",
    "            print('Training val_acc over epoch %s: %s '%(times,float(val_acc)))\n",
    "            val_acc_metric.reset_states()\n",
    "    \n",
    "    \n",
    "    def save_model(self):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        pass\n",
    "    \n",
    "    def eval_model(self):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        pass\n",
    "    \n",
    "    def view_train(self):\n",
    "        if not self.history:\n",
    "            print(\"Model has not been trained, train it first\")\n",
    "            return\n",
    "        \n",
    "        train=self.history.history[\"loss\"]\n",
    "        valid=self.history.history[\"val_loss\"]\n",
    "        name='loss'\n",
    "        plt.title('The %s with epoch runs'%name,fontsize=30)\n",
    "        plt.xlabel('epoch',fontsize=20)\n",
    "        plt.ylabel(name,fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.plot(train,label=name)\n",
    "        plt.plot(valid,label=\"val_\"+name)\n",
    "        plt.legend()\n",
    "        plt.gcf().set_size_inches(15,4)\n",
    "        plt.show()\n",
    "    #model=K.models.Model(inputs=[input_layer],outputs=[output_layer])\n",
    "    #model.compile('adam',loss='categorical_crossentropy',metrics=metrics)\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "train_config=TrainingConfig()\n",
    "word_embedding=WordEmbedding()\n",
    "model=AD_LSTM(train_config,word_embedding,freq)\n",
    "ss=model.design_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self,config):\n",
    "        self.config=config\n",
    "    def load(self):\n",
    "        L=[]\n",
    "        for item in self.config.source:\n",
    "            L.append(np.load(item,allow_pickle=True, fix_imports=True))\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data=Dataset(DataSourceConfig)\n",
    "train_data,test_data,train_labels,test_labels=data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "train_counter=Counter()\n",
    "for seq in train_data:\n",
    "    train_counter.update(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "不能算padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter[0]=0\n",
    "\n",
    "total_words=sum(train_counter.values())\n",
    "\n",
    "emb_count=10000\n",
    "\n",
    "freq=np.zeros(emb_count)\n",
    "\n",
    "for i,j in train_counter.items():\n",
    "    freq[i]=j/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_config=TrainingConfig()\n",
    "word_embedding=WordEmbedding()\n",
    "model=AD_LSTM(train_config,word_embedding,freq)\n",
    "model.design_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 256)\n",
      "(2500, 256)\n",
      " Training loss (for one batch) at step 150: 0.6602101325988774\n",
      "Training     acc over epoch 0: 0.5202666521072388\n",
      "Training val_acc over epoch 0: 0.5623999834060669 \n",
      " Training loss (for one batch) at step 150: 0.5542582273483276\n",
      "Training     acc over epoch 1: 0.5946666598320007\n",
      "Training val_acc over epoch 1: 0.5884000062942505 \n",
      " Training loss (for one batch) at step 150: 0.5193167924880981\n",
      "Training     acc over epoch 2: 0.6478222012519836\n",
      "Training val_acc over epoch 2: 0.7260000109672546 \n",
      " Training loss (for one batch) at step 150: 0.3924117684364319\n",
      "Training     acc over epoch 3: 0.8291110992431641\n",
      "Training val_acc over epoch 3: 0.8051999807357788 \n",
      " Training loss (for one batch) at step 150: 0.33916124701499945\n",
      "Training     acc over epoch 4: 0.859155535697937\n",
      "Training val_acc over epoch 4: 0.8223999738693237 \n",
      " Training loss (for one batch) at step 150: 0.34470617771148686\n",
      "Training     acc over epoch 5: 0.8307999968528748\n",
      "Training val_acc over epoch 5: 0.8076000213623047 \n",
      " Training loss (for one batch) at step 150: 0.28359857201576233\n",
      "Training     acc over epoch 6: 0.8558666706085205\n",
      "Training val_acc over epoch 6: 0.8004000186920166 \n",
      " Training loss (for one batch) at step 150: 0.28044360876083374\n",
      "Training     acc over epoch 7: 0.861644446849823\n",
      "Training val_acc over epoch 7: 0.7196000218391418 \n",
      " Training loss (for one batch) at step 150: 0.5075203776359558\n",
      "Training     acc over epoch 8: 0.6363111138343811\n",
      "Training val_acc over epoch 8: 0.7767999768257141 \n",
      " Training loss (for one batch) at step 150: 0.45589104294776917\n",
      "Training     acc over epoch 9: 0.6607555747032166\n",
      "Training val_acc over epoch 9: 0.7943999767303467 \n",
      " Training loss (for one batch) at step 150: 0.31437546014785767\n",
      "Training     acc over epoch 10: 0.8473777770996094\n",
      "Training val_acc over epoch 10: 0.8015999794006348 \n",
      " Training loss (for one batch) at step 150: 0.3022736608982086\n",
      "Training     acc over epoch 11: 0.8539555668830872\n",
      "Training val_acc over epoch 11: 0.7947999835014343 \n",
      " Training loss (for one batch) at step 150: 0.3108421862125397\n",
      "Training     acc over epoch 12: 0.8624444603919983\n",
      "Training val_acc over epoch 12: 0.7839999794960022 \n",
      " Training loss (for one batch) at step 150: 0.27594441175460815\n",
      "Training     acc over epoch 13: 0.8553777933120728\n",
      "Training val_acc over epoch 13: 0.7972000241279602 \n",
      " Training loss (for one batch) at step 150: 0.28647762537002563\n",
      "Training     acc over epoch 14: 0.8670666813850403\n",
      "Training val_acc over epoch 14: 0.8051999807357788 \n",
      " Training loss (for one batch) at step 150: 0.29298320412635803\n",
      "Training     acc over epoch 15: 0.8625777959823608\n",
      "Training val_acc over epoch 15: 0.8108000159263611 \n",
      " Training loss (for one batch) at step 150: 0.29608732461929327\n",
      "Training     acc over epoch 16: 0.8699555397033691\n",
      "Training val_acc over epoch 16: 0.8068000078201294 \n",
      " Training loss (for one batch) at step 150: 0.29546710848808294\n",
      "Training     acc over epoch 17: 0.8727555274963379\n",
      "Training val_acc over epoch 17: 0.8100000023841858 \n",
      " Training loss (for one batch) at step 150: 0.3099389374256134\n",
      "Training     acc over epoch 18: 0.8327999711036682\n",
      "Training val_acc over epoch 18: 0.7904000282287598 \n",
      " Training loss (for one batch) at step 150: 0.2660559415817261\n",
      "Training     acc over epoch 19: 0.8341333270072937\n",
      "Training val_acc over epoch 19: 0.7991999983787537 \n",
      " Training loss (for one batch) at step 150: 0.50062495470047004\n",
      "Training     acc over epoch 20: 0.6959111094474792\n",
      "Training val_acc over epoch 20: 0.7372000217437744 \n",
      " Training loss (for one batch) at step 150: 0.5386016964912415\n",
      "Training     acc over epoch 21: 0.6622666716575623\n",
      "Training val_acc over epoch 21: 0.6751999855041504 \n",
      " Training loss (for one batch) at step 150: 0.40971291065216064\n",
      "Training     acc over epoch 22: 0.748977780342102\n",
      "Training val_acc over epoch 22: 0.782800018787384 \n",
      " Training loss (for one batch) at step 150: 0.3160226345062256\n",
      "Training     acc over epoch 23: 0.8567110896110535\n",
      "Training val_acc over epoch 23: 0.8288000226020813 \n",
      " Training loss (for one batch) at step 150: 0.24925588071346283\n",
      "Training     acc over epoch 24: 0.9008888602256775\n",
      "Training val_acc over epoch 24: 0.8515999913215637 \n",
      " Training loss (for one batch) at step 150: 0.19543431699275972\n",
      "Training     acc over epoch 25: 0.9273777604103088\n",
      "Training val_acc over epoch 25: 0.8543999791145325 \n",
      " Training loss (for one batch) at step 150: 0.14106993377208715\n",
      "Training     acc over epoch 26: 0.9459111094474792\n",
      "Training val_acc over epoch 26: 0.8579999804496765 \n",
      " Training loss (for one batch) at step 150: 0.1246601790189743\n",
      "Training     acc over epoch 27: 0.9603111147880554\n",
      "Training val_acc over epoch 27: 0.8519999980926514 \n",
      " Training loss (for one batch) at step 150: 0.12788578867912292\n",
      "Training     acc over epoch 28: 0.960622251033783\n",
      "Training val_acc over epoch 28: 0.8324000239372253 \n",
      " Training loss (for one batch) at step 150: 0.10692316293716432\n",
      "Training     acc over epoch 29: 0.9573777914047241\n",
      "Training val_acc over epoch 29: 0.8500000238418579 \n",
      " Training loss (for one batch) at step 150: 0.090229213237762454\n",
      "Training     acc over epoch 30: 0.967199981212616\n",
      "Training val_acc over epoch 30: 0.8644000291824341 \n",
      " Training loss (for one batch) at step 150: 0.059639289975166325\n",
      "Training     acc over epoch 31: 0.9771999716758728\n",
      "Training val_acc over epoch 31: 0.864799976348877 \n",
      " Training loss (for one batch) at step 150: 0.05571882799267769\n",
      "Training     acc over epoch 32: 0.9846222400665283\n",
      "Training val_acc over epoch 32: 0.8619999885559082 \n",
      " Training loss (for one batch) at step 150: 0.041247468441724787\n",
      "Training     acc over epoch 33: 0.98862224817276\n",
      "Training val_acc over epoch 33: 0.86080002784729 \n",
      " Training loss (for one batch) at step 150: 0.03044278174638748\n",
      "Training     acc over epoch 34: 0.9906666874885559\n",
      "Training val_acc over epoch 34: 0.8604000210762024 \n",
      " Training loss (for one batch) at step 150: 0.022744446992874146\n",
      "Training     acc over epoch 35: 0.9919111132621765\n",
      "Training val_acc over epoch 35: 0.8600000143051147 \n",
      " Training loss (for one batch) at step 150: 0.019353345036506653\n",
      "Training     acc over epoch 36: 0.9923555850982666\n",
      "Training val_acc over epoch 36: 0.8592000007629395 \n",
      " Training loss (for one batch) at step 150: 0.017208026722073555\n",
      "Training     acc over epoch 37: 0.9929777979850769\n",
      "Training val_acc over epoch 37: 0.8604000210762024 \n",
      " Training loss (for one batch) at step 150: 0.015642507001757622\n",
      "Training     acc over epoch 38: 0.9931111335754395\n",
      "Training val_acc over epoch 38: 0.8583999872207642 \n",
      " Training loss (for one batch) at step 150: 0.013185448013246065\n",
      "Training     acc over epoch 39: 0.992888867855072\n",
      "Training val_acc over epoch 39: 0.8604000210762024 \n",
      " Training loss (for one batch) at step 150: 0.033223699778318405\n",
      "Training     acc over epoch 40: 0.9932000041007996\n",
      "Training val_acc over epoch 40: 0.8503999710083008 \n",
      " Training loss (for one batch) at step 150: 0.019310744479298592\n",
      "Training     acc over epoch 41: 0.991955578327179\n",
      "Training val_acc over epoch 41: 0.8587999939918518 \n",
      " Training loss (for one batch) at step 150: 0.027998864650726328\n",
      "Training     acc over epoch 42: 0.9923111200332642\n",
      "Training val_acc over epoch 42: 0.8632000088691711 \n",
      " Training loss (for one batch) at step 150: 0.014564216136932373\n",
      "Training     acc over epoch 43: 0.9917333126068115\n",
      "Training val_acc over epoch 43: 0.8600000143051147 \n",
      " Training loss (for one batch) at step 150: 0.013943426311016083\n",
      "Training     acc over epoch 44: 0.9910222291946411\n",
      "Training val_acc over epoch 44: 0.8600000143051147 \n",
      " Training loss (for one batch) at step 150: 0.047249652445316315\n",
      "Training     acc over epoch 45: 0.9891999959945679\n",
      "Training val_acc over epoch 45: 0.8575999736785889 \n",
      " Training loss (for one batch) at step 150: 0.011539174243807793\n",
      "Training     acc over epoch 46: 0.9875110983848572\n",
      "Training val_acc over epoch 46: 0.8604000210762024 \n",
      " Training loss (for one batch) at step 150: 0.009170499630272388\n",
      "Training     acc over epoch 47: 0.9897333383560181\n",
      "Training val_acc over epoch 47: 0.8587999939918518 \n",
      " Training loss (for one batch) at step 150: 0.008630406111478806\n",
      "Training     acc over epoch 48: 0.9946222305297852\n",
      "Training val_acc over epoch 48: 0.8611999750137329 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training loss (for one batch) at step 150: 0.0068917665630578995\n",
      "Training     acc over epoch 49: 0.996222198009491\n",
      "Training val_acc over epoch 49: 0.8619999885559082 \n"
     ]
    }
   ],
   "source": [
    "model.train_model_custom(train_data,train_labels,adv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "def norm_by_freq(self, freq):\n",
    "        word_embs = self.emb.embedding\n",
    "        mean = F.sum(freq * word_embs, axis=0, keepdims=True)\n",
    "        mean = F.broadcast_to(mean, word_embs.shape)\n",
    "        var = F.sum(freq * ((word_embs - mean) ** 2), axis=0, keepdims=True)\n",
    "        var = F.broadcast_to(var, word_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 256)\n",
      "(2500, 256)\n",
      " Training loss (for one batch) at step 150: 0.6806985735893259\n",
      "Training     acc over epoch 0: 0.5132444500923157\n",
      "Training val_acc over epoch 0: 0.5479999780654907 \n",
      " Training loss (for one batch) at step 150: 0.6297447681427002\n",
      "Training     acc over epoch 1: 0.5797333121299744\n",
      "Training val_acc over epoch 1: 0.5888000130653381 \n",
      " Training loss (for one batch) at step 150: 0.47636282444000244\n",
      "Training     acc over epoch 2: 0.6672444343566895\n",
      "Training val_acc over epoch 2: 0.7868000268936157 \n",
      " Training loss (for one batch) at step 150: 0.46456941962242126\n",
      "Training     acc over epoch 3: 0.8018222451210022\n",
      "Training val_acc over epoch 3: 0.5616000294685364 \n",
      " Training loss (for one batch) at step 150: 0.6231471300125122\n",
      "Training     acc over epoch 4: 0.5627111196517944\n",
      "Training val_acc over epoch 4: 0.5595999956130981 \n",
      " Training loss (for one batch) at step 150: 0.5995634198188782\n",
      "Training     acc over epoch 5: 0.6024888753890991\n",
      "Training val_acc over epoch 5: 0.5616000294685364 \n",
      " Training loss (for one batch) at step 150: 0.4519086778163919\n",
      "Training     acc over epoch 6: 0.690488874912262\n",
      "Training val_acc over epoch 6: 0.7875999808311462 \n",
      " Training loss (for one batch) at step 150: 0.48209962248802185\n",
      "Training     acc over epoch 7: 0.7365333437919617\n",
      "Training val_acc over epoch 7: 0.7735999822616577 \n",
      " Training loss (for one batch) at step 150: 0.41770362854003906\n",
      "Training     acc over epoch 8: 0.7415111064910889\n",
      "Training val_acc over epoch 8: 0.7692000269889832 \n",
      " Training loss (for one batch) at step 150: 0.44011580944061283\n",
      "Training     acc over epoch 9: 0.7840444445610046\n",
      "Training val_acc over epoch 9: 0.7784000039100647 \n",
      " Training loss (for one batch) at step 150: 0.38508468866348267\n",
      "Training     acc over epoch 10: 0.783822238445282\n",
      "Training val_acc over epoch 10: 0.8019999861717224 \n",
      " Training loss (for one batch) at step 150: 0.3815563917160034\n",
      "Training     acc over epoch 11: 0.8090222477912903\n",
      "Training val_acc over epoch 11: 0.8087999820709229 \n",
      " Training loss (for one batch) at step 150: 0.5115463733673096\n",
      "Training     acc over epoch 12: 0.8039555549621582\n",
      "Training val_acc over epoch 12: 0.7480000257492065 \n",
      " Training loss (for one batch) at step 150: 0.42687177658081055\n",
      "Training     acc over epoch 13: 0.7884888648986816\n",
      "Training val_acc over epoch 13: 0.7807999849319458 \n",
      " Training loss (for one batch) at step 150: 0.4275542199611664\n",
      "Training     acc over epoch 14: 0.8042222261428833\n",
      "Training val_acc over epoch 14: 0.7900000214576721 \n",
      " Training loss (for one batch) at step 150: 0.56490731239318857\n",
      "Training     acc over epoch 15: 0.7689777612686157\n",
      "Training val_acc over epoch 15: 0.7487999796867371 \n",
      " Training loss (for one batch) at step 150: 0.45903754234313965\n",
      "Training     acc over epoch 16: 0.8149333596229553\n",
      "Training val_acc over epoch 16: 0.8011999726295471 \n",
      " Training loss (for one batch) at step 150: 0.37702012062072754\n",
      "Training     acc over epoch 17: 0.8316888809204102\n",
      "Training val_acc over epoch 17: 0.8176000118255615 \n",
      " Training loss (for one batch) at step 150: 0.43377411365509033\n",
      "Training     acc over epoch 18: 0.8457333445549011\n",
      "Training val_acc over epoch 18: 0.8252000212669373 \n",
      " Training loss (for one batch) at step 150: 0.42027604579925537\n",
      "Training     acc over epoch 19: 0.8373333215713501\n",
      "Training val_acc over epoch 19: 0.823199987411499 \n",
      " Training loss (for one batch) at step 150: 0.3347185254096985\n",
      "Training     acc over epoch 20: 0.8411111235618591\n",
      "Training val_acc over epoch 20: 0.8227999806404114 \n",
      " Training loss (for one batch) at step 150: 0.3697933554649353\n",
      "Training     acc over epoch 21: 0.8301777839660645\n",
      "Training val_acc over epoch 21: 0.8199999928474426 \n",
      " Training loss (for one batch) at step 150: 0.3793153762817383\n",
      "Training     acc over epoch 22: 0.8477333188056946\n",
      "Training val_acc over epoch 22: 0.8295999765396118 \n",
      " Training loss (for one batch) at step 150: 0.3659217655658722\n",
      "Training     acc over epoch 23: 0.8579999804496765\n",
      "Training val_acc over epoch 23: 0.8356000185012817 \n",
      " Training loss (for one batch) at step 150: 0.36485904455184937\n",
      "Training     acc over epoch 24: 0.8632888793945312\n",
      "Training val_acc over epoch 24: 0.8335999846458435 \n",
      " Training loss (for one batch) at step 150: 0.3573681116104126\n",
      "Training     acc over epoch 25: 0.8567110896110535\n",
      "Training val_acc over epoch 25: 0.8227999806404114 \n",
      " Training loss (for one batch) at step 150: 0.36266237497329716\n",
      "Training     acc over epoch 26: 0.8584444522857666\n",
      "Training val_acc over epoch 26: 0.8284000158309937 \n",
      " Training loss (for one batch) at step 150: 0.3981577754020691\n",
      "Training     acc over epoch 27: 0.8432444334030151\n",
      "Training val_acc over epoch 27: 0.8184000253677368 \n",
      " Training loss (for one batch) at step 150: 0.37592771649360657\n",
      "Training     acc over epoch 28: 0.8575999736785889\n",
      "Training val_acc over epoch 28: 0.819599986076355 \n",
      " Training loss (for one batch) at step 150: 0.33001166582107544\n",
      "Training     acc over epoch 29: 0.8555999994277954\n",
      "Training val_acc over epoch 29: 0.8240000009536743 \n",
      " Training loss (for one batch) at step 150: 0.36437082290649414\n",
      "Training     acc over epoch 30: 0.8645777702331543\n",
      "Training val_acc over epoch 30: 0.8235999941825867 \n",
      " Training loss (for one batch) at step 150: 0.33581632375717163\n",
      "Training     acc over epoch 31: 0.8644888997077942\n",
      "Training val_acc over epoch 31: 0.8327999711036682 \n",
      " Training loss (for one batch) at step 150: 0.35668000578880315\n",
      "Training     acc over epoch 32: 0.853866696357727\n",
      "Training val_acc over epoch 32: 0.8212000131607056 \n",
      " Training loss (for one batch) at step 150: 0.5357732772827148\n",
      "Training     acc over epoch 33: 0.6348000168800354\n",
      "Training val_acc over epoch 33: 0.6051999926567078 \n",
      " Training loss (for one batch) at step 150: 0.30255407094955444\n",
      "Training     acc over epoch 34: 0.7810222506523132\n",
      "Training val_acc over epoch 34: 0.8515999913215637 \n",
      " Training loss (for one batch) at step 150: 0.26238441467285156\n",
      "Training     acc over epoch 35: 0.8809333443641663\n",
      "Training val_acc over epoch 35: 0.8708000183105469 \n",
      " Training loss (for one batch) at step 150: 0.22307768464088443\n",
      "Training     acc over epoch 36: 0.9016000032424927\n",
      "Training val_acc over epoch 36: 0.8791999816894531 \n",
      " Training loss (for one batch) at step 150: 0.19896796345710754\n",
      "Training     acc over epoch 37: 0.913955569267273\n",
      "Training val_acc over epoch 37: 0.8784000277519226 \n",
      " Training loss (for one batch) at step 150: 0.17619690299034126\n",
      "Training     acc over epoch 38: 0.92448890209198\n",
      "Training val_acc over epoch 38: 0.8840000033378601 \n",
      " Training loss (for one batch) at step 150: 0.13969662785530098\n",
      "Training     acc over epoch 39: 0.9281777739524841\n",
      "Training val_acc over epoch 39: 0.8823999762535095 \n",
      " Training loss (for one batch) at step 150: 0.19065198302268982\n",
      "Training     acc over epoch 40: 0.9380444288253784\n",
      "Training val_acc over epoch 40: 0.8840000033378601 \n",
      " Training loss (for one batch) at step 150: 0.12150478363037118\n",
      "Training     acc over epoch 41: 0.9422222375869751\n",
      "Training val_acc over epoch 41: 0.88919997215271 \n",
      " Training loss (for one batch) at step 150: 0.17912189662456512\n",
      "Training     acc over epoch 42: 0.9448444247245789\n",
      "Training val_acc over epoch 42: 0.8844000101089478 \n",
      " Training loss (for one batch) at step 150: 0.18132914602756557\n",
      "Training     acc over epoch 43: 0.9484888911247253\n",
      "Training val_acc over epoch 43: 0.8880000114440918 \n",
      " Training loss (for one batch) at step 150: 0.18002104759216309\n",
      "Training     acc over epoch 44: 0.9515555500984192\n",
      "Training val_acc over epoch 44: 0.8876000046730042 \n",
      " Training loss (for one batch) at step 150: 0.11192061752080917\n",
      "Training     acc over epoch 45: 0.9563999772071838\n",
      "Training val_acc over epoch 45: 0.8840000033378601 \n",
      " Training loss (for one batch) at step 150: 0.0798426941037178\n",
      "Training     acc over epoch 46: 0.9572444558143616\n",
      "Training val_acc over epoch 46: 0.8795999884605408 \n",
      " Training loss (for one batch) at step 150: 0.16068442165851593\n",
      "Training     acc over epoch 47: 0.9587110877037048\n",
      "Training val_acc over epoch 47: 0.878000020980835 \n",
      " Training loss (for one batch) at step 150: 0.090977601706981664\n",
      "Training     acc over epoch 48: 0.9623110890388489\n",
      "Training val_acc over epoch 48: 0.8835999965667725 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training loss (for one batch) at step 150: 0.065217532217502695\n",
      "Training     acc over epoch 49: 0.9647555351257324\n",
      "Training val_acc over epoch 49: 0.881600022315979 \n"
     ]
    }
   ],
   "source": [
    "model.train_model_custom(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.train_model(x=train_data,y=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

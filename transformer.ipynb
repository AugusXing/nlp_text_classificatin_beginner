{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "#ipdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.layers as L\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig():\n",
    "    epochs = 5\n",
    "    batch_size= 128\n",
    "    learningRate=0.001\n",
    "    valRate=0.1\n",
    "    loss=['sparse_categorical_crossentropy']\n",
    "    metrics=['accuracy']\n",
    "    optimizer='adam'\n",
    "    \n",
    "class WordEmbedding():\n",
    "    sequenceLength=256\n",
    "    wordCount=10000\n",
    "    wordDim=48\n",
    "\n",
    "class DataSourceConfig():\n",
    "    source=[\"./dataset/imdb_train_data.npy\",\n",
    "           \"./dataset/imdb_test_data.npy\",\n",
    "           \"./dataset/imdb_train_labels.npy\",\n",
    "           \"./dataset/imdb_test_labels.npy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def get_angles(pos,i,d_model):\n",
    "    angle_rates=1/np.power(10000,(2*(i//2))/np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "def positional_encoding(position,d_model):\n",
    "    angle_rads=get_angles(np.arange(position)[:,np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis,:],\n",
    "                          d_model\n",
    "                         )\n",
    "    angle_rads[:,0::2]=np.sin(angle_rads[:,0::2])\n",
    "    angle_rads[:,1::2]=np.cos(angle_rads[:,1::2])\n",
    "    #ipdb.set_trace()\n",
    "    pos_encoding=angle_rads[np.newaxis,...]\n",
    "    return tf.cast(pos_encoding,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq=tf.cast(tf.math.equal(seq,0),tf.float32)\n",
    "    # 添加额外的维度来将填充加到\n",
    "    # 注意力对数（logits）。\n",
    "    return seq[:,tf.newaxis,tf.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,mask):\n",
    "    matmul_qk=tf.matmul(q,k,transpose_b=True)#(batch_size,seq_q,seq_k)\n",
    "    #缩放\n",
    "    dk=tf.cast(tf.shape(k)[-1],tf.float32)\n",
    "    scaled_attention_logits=matmul_qk/tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits+=(mask*-1e9)\n",
    "        \n",
    "    attention_weights=tf.nn.softmax(scaled_attention_logits,axis=-1)\n",
    "    output=tf.matmul(attention_weights,v)\n",
    "    \n",
    "    return output,attention_weights    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"分拆最后一个维度到 (num_heads, depth).\n",
    "    转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(2,activation='softmax')  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    \n",
    "    self.fc_layer=fc_layer(dff)\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training=True, mask=None):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # 将嵌入和位置编码相加。\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    x=self.fc_layer(x)\n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    def __init__(self,config,wordEmbedding):\n",
    "        \n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.config=config\n",
    "        self.emb=wordEmbedding\n",
    "        self.name=\"TextCNN\"\n",
    "        \n",
    "    \n",
    "    def design_model(self,hiden_size=64):\n",
    "        \n",
    "        sample_encoder = Encoder(num_layers=2, d_model=48, num_heads=8, \n",
    "                         dff=hiden_size, input_vocab_size=self.emb.wordCount,\n",
    "                         maximum_position_encoding=self.emb.sequenceLength)\n",
    "        self.model=sample_encoder\n",
    "        return self.model\n",
    "    \n",
    "    def compile_model(self):\n",
    "        #assert()\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "            return\n",
    "        self.model.compile(optimizer=self.config.optimizer,loss=self.config.loss,metrics=self.config.metrics)\n",
    "\n",
    "    def train_model(self,x,y):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "            return\n",
    "        self.compile_model()\n",
    "        self.history=self.model.fit(x=x,y=y,batch_size=self.config.batch_size,epochs=self.config.epochs,validation_split=self.config.valRate)\n",
    "        #self.model.compile()\n",
    "    def train_model_custom(self,x,y):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.1)\n",
    "        print(x_train.shape)\n",
    "        print(x_val.shape)\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(self.config.batch_size)\n",
    "        \n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(self.config.batch_size)\n",
    "\n",
    "        #loss\n",
    "        loss_fn=K.losses.get(self.config.loss[0])\n",
    "        #opt\n",
    "        optimizer=K.optimizers.Adam()\n",
    "        #metric\n",
    "        train_acc_metric=K.metrics.SparseCategoricalAccuracy()#self.config.metrics[0])\n",
    "        val_acc_metric  =K.metrics.SparseCategoricalAccuracy()#self.config.metrics[0])\n",
    "        \n",
    "        for times in range(self.config.epochs):\n",
    "            step=0\n",
    "            # 训练过程\n",
    "            for x_batch_train, y_batch_train in train_dataset:\n",
    "                with tf.GradientTape() as tp:\n",
    "                    logits=self.model(x_batch_train)\n",
    "                    loss=loss_fn(y_batch_train,logits)\n",
    "                grads=tp.gradient(loss,self.model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "                train_acc_metric.update_state(y_batch_train,logits)\n",
    "                \n",
    "                if step % 50 == 0:\n",
    "                    print('\\r','Training loss (for one batch) at step %s: %s' % (step, float(tf.reduce_mean(loss))),end=\"\",flush=True)\n",
    "                    #print('Seen so far: %s samples' % ((step + 1) * self.config.batch_size))\n",
    "                step+=1\n",
    "            print()\n",
    "            # 每个周期结束看一次\n",
    "            train_acc = train_acc_metric.result()\n",
    "            print('Training     acc over epoch %s: %s' % (times,float(train_acc)))\n",
    "            # Reset training metrics at the end of each epoch\n",
    "            train_acc_metric.reset_states()\n",
    "            \n",
    "            #验证过程\n",
    "            for x_batch_val, y_batch_val in val_dataset:\n",
    "                val_logits=self.model(x_batch_val)\n",
    "                val_acc_metric.update_state(y_batch_val,val_logits)\n",
    "            \n",
    "            val_acc=val_acc_metric.result()\n",
    "            print('Training val_acc over epoch %s: %s '%(times,float(val_acc)))\n",
    "            val_acc_metric.reset_states()\n",
    "\n",
    "            \n",
    "    def save_model(self):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        pass\n",
    "    \n",
    "    def eval_model(self):\n",
    "        if not self.model:\n",
    "            print(\"Call design_modelXX() to build the model first.\")\n",
    "        pass\n",
    "    \n",
    "    def view_train(self):\n",
    "        if not self.history:\n",
    "            print(\"Model has not been trained, train it first\")\n",
    "            return\n",
    "        \n",
    "        train=self.history.history[\"loss\"]\n",
    "        valid=self.history.history[\"val_loss\"]\n",
    "        name='loss'\n",
    "        plt.title('The %s with epoch runs'%name,fontsize=30)\n",
    "        plt.xlabel('epoch',fontsize=20)\n",
    "        plt.ylabel(name,fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.plot(train,label=name)\n",
    "        plt.plot(valid,label=\"val_\"+name)\n",
    "        plt.legend()\n",
    "        plt.gcf().set_size_inches(15,4)\n",
    "        plt.show()\n",
    "    #model=K.models.Model(inputs=[input_layer],outputs=[output_layer])\n",
    "    #model.compile('adam',loss='categorical_crossentropy',metrics=metrics)\n",
    "    #return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config=TrainingConfig()\n",
    "word_embedding=WordEmbedding()\n",
    "model=Transformer(train_config,word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self,config):\n",
    "        self.config=config\n",
    "    def load(self):\n",
    "        L=[]\n",
    "        for item in self.config.source:\n",
    "            L.append(np.load(item,allow_pickle=True, fix_imports=True))\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Dataset(DataSourceConfig)\n",
    "train_data,test_data,train_labels,test_labels=data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Encoder at 0x153c60450>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.design_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 256)\n",
      "(2500, 256)\n",
      " Training loss (for one batch) at step 50: 0.7049741744995117"
     ]
    }
   ],
   "source": [
    "model.train_model_custom(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
